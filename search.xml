<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Hadoop Streaming 使用总结</title>
    <url>/2020/03/26/hadoop-streaming/</url>
    <content><![CDATA[<p>Hadoop Streaming 是 Hadoop 提供的一个工具， 用户可以使用它来创建和运行一类特殊的 MapReduce 任务， 这些 MR 任务可以使用任何可执行文件或脚本作为 mapper 和 reducer。</p><p>比如，简单的 word count 任务可使用 Hadoop Streaming 简单写为：</p><a id="more"></a>

<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \</span><br><span class="line">    -input /tmp/wordcount/input \</span><br><span class="line">    -output /tmp/wordcount/output \</span><br><span class="line">    -mapper /bin/cat \</span><br><span class="line">    -reducer /usr/bin/wc</span><br></pre></td></tr></table></figure>

<h2 id="工作原理"><a href="#工作原理" class="headerlink" title="工作原理"></a>工作原理</h2><p> Hadoop Streaming 会创建一个 MR 任务， 然后将任务提交到集群上执行，同时监控这个任务的整个执行过程。如果 mapper 和 reducer 都是可执行文件，streaming 程序会使用 PipeMapper 和 PipeReducer 来做一个类似代理的 Mapper 和 Reducer，它们负责启动实际的 mapper 和 reducer 可执行文件，然后从 HDFS 读取输入数据，再一行一行写入到可执行文件进程的标准输入，同时读取可执行文件进程处理完数据后输出到标准输出的数据，将其写出到 Mapper 和 Reducer 真正的输出中。</p>
<p>以一个没有 reduce 阶段的 Streaming 程序为例，其 Mapper 简要运行流程可见下图：</p>
<p><img src="hadoop-streaming.jpg" alt="Hadoop Streaming Mapper 运行流程"></p>
<p>PipeMapper 在启动 <code>mapper.sh</code> 后， 不断重复 2-7 （一次 map ）过程，直到所有数据处理完成。</p>
<p>与 PipeMapper 类似，PipeReducer 会将从 map 端 shuffle 过来数据，一行行的写到 reducer.sh 进程的标准输入，然后收集 reducer.sh 进程的标准输出，最终写出到 hdfs output。</p>
<p>以上就是是 MapReduce 框架和 streaming mapper/reducer 之间的基本处理流程。所以，用户在编写Streaming 程序的 mapper 和 reducer 时，只需要从不断 stdin 中一行行读取数据，处理然后输出到标准输出中即可。</p>
<p>同时，用户也可以使用 java 类作为 mapper 或者 reducer 。上面的例子与这里的代码等价：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \</span><br><span class="line">    -input /tmp/wordcount/input \</span><br><span class="line">    -output /tmp/wordcount/output \</span><br><span class="line">    -mapper org.apache.hadoop.mapred.lib.IdentityMapper \</span><br><span class="line">    -reducer /usr/bin/wc</span><br></pre></td></tr></table></figure>

<p>用户也可以设定 <code>stream.non.zero.exit.is.failure</code> <code>true</code> 或 <code>false</code> 来表明 streaming task 的返回值非零时是 Failure 还是 Success。默认情况，streaming task 返回非零时表示失败。</p>
<h2 id="参数配置"><a href="#参数配置" class="headerlink" title="参数配置"></a>参数配置</h2><p>Hadoop Streaming 支持 Streaming 命令参数配置以及 Hadoop 通用的参数配置。 常规命令行语法如下所示。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hadoop command [genericOptions] [streamingOptions]</span><br></pre></td></tr></table></figure>

<p><strong>注意：确保通用参数放置在 Streaming 的参数配置之前，否则命令将会失败。 具体可查看后续的示例。</strong></p>
<h3 id="Streaming-参数配置"><a href="#Streaming-参数配置" class="headerlink" title="Streaming 参数配置"></a>Streaming 参数配置</h3><p>Streaming 命令参数配置具体如下表所示：</p>
<table>
<thead>
<tr>
<th>参数</th>
<th>必选</th>
<th>意义</th>
</tr>
</thead>
<tbody><tr>
<td>-input   directoryname or filename</td>
<td>是</td>
<td>设置输入数据路径，可以是文件或者目录。可通过重复配置，添加多个输入路径</td>
</tr>
<tr>
<td>-output  directoryname</td>
<td>是</td>
<td>设置指定输出数据路径，必须是目录。输出路径只能有一个</td>
</tr>
<tr>
<td>-mapper  executable or JavaClassName</td>
<td>是</td>
<td>设置可执行的 mapper</td>
</tr>
<tr>
<td>-reducer executable or JavaClassName</td>
<td>是</td>
<td>设置可执行的 reducer</td>
</tr>
<tr>
<td>-file filename</td>
<td>否</td>
<td>设置需要同步到计算节点的文件，可以使可执行的 mapper，reducer 或 combiner 文件在计算节点本地可用。可以通过重复配置，同步多个文件</td>
</tr>
<tr>
<td>-inputformat JavaClassName</td>
<td>否</td>
<td>设置 InputFormat，用来将输入文件读取成 key/value 对，如果未设置默认使用 TextInputFormat</td>
</tr>
<tr>
<td>-outputformat JavaClassName</td>
<td>否</td>
<td>设置 OutputFormat，用来将输出的 key/value 对写出到输出文件，如果未设置默认使用 TextOutputFormat</td>
</tr>
<tr>
<td>-partitioner JavaClassName</td>
<td>否</td>
<td>设置 Partitioner， 用来根据 key 确定数据应该指派到的 reduce</td>
</tr>
<tr>
<td>-combiner streamingCommand or JavaClassName</td>
<td>否</td>
<td>设置 Combiner，用来在 mapper 端归并 mapper 的输出</td>
</tr>
<tr>
<td>-cmdenv name=value</td>
<td>否</td>
<td>设置环境变量，可以在 mapper 或者 reducer 运行时获取</td>
</tr>
<tr>
<td>-inputreader</td>
<td>否</td>
<td>设置 InputReader 类，用于读取输入数据，取代 InputFormat Class</td>
</tr>
<tr>
<td>-verbose</td>
<td>否</td>
<td>设置是否输出日志</td>
</tr>
<tr>
<td>-lazyOutput</td>
<td>否</td>
<td>设置是否延迟输出</td>
</tr>
<tr>
<td>-numReduceTasks</td>
<td>否</td>
<td>设置 reducer 任务数量</td>
</tr>
<tr>
<td>-mapdebug</td>
<td>否</td>
<td>设置 mapper debug 脚本，在 mapper 任务运行失败时执行</td>
</tr>
<tr>
<td>-reducedebug</td>
<td>否</td>
<td>设置 reducer debug 脚本， reducer 任务运行失败时执行</td>
</tr>
</tbody></table>
<h3 id="通用参数配置"><a href="#通用参数配置" class="headerlink" title="通用参数配置"></a>通用参数配置</h3><p>Streaming 任务同时支持 Hadoop 通用的参数配置，主要的参数配置有以下几个：</p>
<table>
<thead>
<tr>
<th>参数</th>
<th>必选</th>
<th>意义</th>
</tr>
</thead>
<tbody><tr>
<td>-conf configuration_file</td>
<td>否</td>
<td>指定配置文件</td>
</tr>
<tr>
<td>-D property=value</td>
<td>否</td>
<td>设置参数的值</td>
</tr>
<tr>
<td>-fs host:port or local</td>
<td>否</td>
<td>指定一个 namenode</td>
</tr>
<tr>
<td>-files</td>
<td>否</td>
<td>指定需要拷贝到集群的文件，多个文件以逗号分隔</td>
</tr>
<tr>
<td>-libjars</td>
<td>否</td>
<td>指定需要添加到任务 classpath 的 jar 文件，多个文件以逗号分隔</td>
</tr>
<tr>
<td>-archives</td>
<td>否</td>
<td>指定需要解压到计算节点的压缩文件，多个文件以逗号分隔</td>
</tr>
</tbody></table>
<h3 id="参数配置示例"><a href="#参数配置示例" class="headerlink" title="参数配置示例"></a>参数配置示例</h3><p>下面我们通过一些示例来展示具体的 Streaming 任务参数配置。</p>
<h4 id="设置-Mapper-Reducer"><a href="#设置-Mapper-Reducer" class="headerlink" title="设置 Mapper Reducer"></a>设置 Mapper Reducer</h4><p>使用可执行文件作为 mapper 和 reducer</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \</span><br><span class="line">  -input myInputDir \</span><br><span class="line">  -output myOutputDir \</span><br><span class="line">  -mapper /usr/bin/cat \</span><br><span class="line">  -reducer /usr/bin/wc</span><br></pre></td></tr></table></figure>

<p>使用 Java Class 作为 mapper，指定 InputFormat</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \</span><br><span class="line">  -input myInputDir \</span><br><span class="line">  -output myOutputDir \</span><br><span class="line">  -inputformat org.apache.hadoop.mapred.KeyValueTextInputFormat \</span><br><span class="line">  -mapper org.apache.hadoop.mapred.lib.IdentityMapper \</span><br><span class="line">  -reducer /usr/bin/wc</span><br></pre></td></tr></table></figure>

<h4 id="提交任务时打包文件"><a href="#提交任务时打包文件" class="headerlink" title="提交任务时打包文件"></a>提交任务时打包文件</h4><p>如上文所述，我们可以指定任意的可执行文件作为 mapper 或者 reducer。在提交 Hadoop Streaming 任务时， 可执行的 mapper 或者 reducer 执行文件并不必已经存在 Hadoop 集群的任意一台机器上。如果不存在，我们只需要在提交任务时的时候使用 <code>-file</code> 参数指定需要的文件，告诉集群在提交任务时将这些文件打包，这样Hadoop 会自动将这些文件打包上传到 Hdfs，并同步到每个计算节点。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \</span><br><span class="line">  -input myInputDir \</span><br><span class="line">  -output myOutputDir \</span><br><span class="line">  -mapper wordcount.py \</span><br><span class="line">  -reducer /usr/bin/wc \</span><br><span class="line">  -file wordcount.py</span><br></pre></td></tr></table></figure>
<p>同时，你也可以指定一些依赖文件，打包上传到集群上,提供给 mapper 或者 reducer 任务使用 </p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \</span><br><span class="line">  -input myInputDir \</span><br><span class="line">  -output myOutputDir \</span><br><span class="line">  -mapper wordcount.py \</span><br><span class="line">  -reducer /usr/bin/wc \</span><br><span class="line">  -file wordcount.py \</span><br><span class="line">  -file dictionary.txt</span><br></pre></td></tr></table></figure>

<h4 id="指定任务的其他插件"><a href="#指定任务的其他插件" class="headerlink" title="指定任务的其他插件"></a>指定任务的其他插件</h4><p>跟普通的 MR 任务一样，我们可以指定任务运行时的一些插件</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">-inputformat JavaClassName</span><br><span class="line">-outputformat JavaClassName</span><br><span class="line">-partitioner JavaClassName</span><br><span class="line">-combiner streamingCommand or JavaClassName</span><br></pre></td></tr></table></figure>

<h4 id="设置环境变量"><a href="#设置环境变量" class="headerlink" title="设置环境变量"></a>设置环境变量</h4><p>我们也可以通过参数设置任务运行时的环境变量，并且可以在 mapper 或者 reducer 运行时获取环境变量的值。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \</span><br><span class="line">  -input myInputDir \</span><br><span class="line">  -output myOutputDir \</span><br><span class="line">  -mapper wordcount.py \</span><br><span class="line">  -reducer /usr/bin/wc \</span><br><span class="line">  -file wordcount.py \</span><br><span class="line">  -cmdenv EXAMPLE_DIR=/home/example/dictionaries/ \</span><br><span class="line">  -cmdenv LOG_LEVEL=debug</span><br></pre></td></tr></table></figure>

<h4 id="设置-reducer-数量"><a href="#设置-reducer-数量" class="headerlink" title="设置 reducer 数量"></a>设置 reducer 数量</h4><p>通常，我们需要为任务设置合适的 reducer 数量，默认的 reducer 数量是 1，具体的 reducer 数量应当根据业务需求以及可使用资源等因素来确定。<br>reducer 数量可以使用 Streaming 配置 <code>-numReduceTasks 10</code> 或者通用参数 <code>-D mapreduce.job.reduces=10</code> 来设置。如下，我们将任务的 reducer 数量设为 10。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \</span><br><span class="line">  -D mapreduce.job.reduces=10</span><br><span class="line">  -input myInputDir \</span><br><span class="line">  -output myOutputDir \</span><br><span class="line">  -mapper /usr/bin/cat \</span><br><span class="line">  -reducer /usr/bin/wc</span><br></pre></td></tr></table></figure>

<p>对于 map-only 的任务，在 map 结束后直接输出结果即可，不需要进行 reduce，这时我们需要将 reducer  数量设为 0。设置  <code>-D mapreduce.job.reduces=0</code> 或者 <code>-reducer=NONE</code> 即可。</p>
<h4 id="依赖大文件或者归档文件"><a href="#依赖大文件或者归档文件" class="headerlink" title="依赖大文件或者归档文件"></a>依赖大文件或者归档文件</h4><p>很多 Streaming 任务在运行时需要依赖某些特定的文件或者环境，比如某些分词任务依赖字典文件，或某个python 实现的机器学习任务依赖指定的 module 。此时，我们就需要将依赖的文件或者归档上传到 HDFS 上， 并使用 <code>-files</code> 和 <code>-archives</code> 选项指定已上传的依赖文件或者归档的 HDFS 路径，任务运行时将会将指定的依赖文件和归档分发到各个计算节点上，任务运行时即可依赖这些文件进行相应的操作。  </p>
<p><strong>注意：<code>-files</code> 和 <code>-archives</code> 选项都是通用选项,需要放在 Streaming 命令配置前，否则会导致任务启动失败</strong></p>
<p><code>-files</code> 选项指定的依赖文件会在任务启动之前分发到当前计算节点上。依赖文件分完成后，会在任务的工作目录里建个软链指向它。<br>如下示例，Hadoop 会在任务的当前工作目录中自动创建名为 dict.txt 的符号链接，指向 dict.txt 的实际复制到的本地路径。任务直接通过软链引用文件即可。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">-files hdfs://host:port/tmp/cache/dict.txt</span><br></pre></td></tr></table></figure>
<p>用户也可以自己指定符号链接的名称，如下示例，将建立名为 dict 的软链。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">-files hdfs://host:port/tmp/cache/dict.txt#dict</span><br></pre></td></tr></table></figure>
<p>对于多个依赖文件，可以用逗号分隔</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">-files hdfs://host:port/tmp/cache/dict.txt,hdfs://host:port/tmp/cache/test.txt</span><br></pre></td></tr></table></figure>

<p><code>-archives</code> 选项指定的归档文件会在任务启动之前分发到当前计算节点上，对于使用某些压缩的归档文件（tar 或 jar）分发到计算节点上后， Hadoop 会自动将其解压到一个目录里，并在任务的工作目录里建个软链指向这个目录。<br>如下示例，Hadoop 会在任务的当前工作目录中自动创建名为 dict.tar.gz 的符号链接，指向 dict.tar.gz 解压到的目录。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">-archives hdfs://host:port/tmp/cache/dict.tar.gz</span><br></pre></td></tr></table></figure>
<p>用户也可以自己指定符号链接的名称，如下示例，将建立名为 mydict 的软链。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">-archives hdfs://host:port/tmp/cache/dict.tar.gz#mydict</span><br></pre></td></tr></table></figure>

<p>指定多个归档依赖文件，可以用逗号分隔。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">-archives hdfs://host:port/tmp/cache/dict.tar.gz,hdfs://host:port/tmp/cache/test.tar.gz</span><br></pre></td></tr></table></figure>

<p>对于任务依赖包含许多文件的目录时，如 python 依赖的某些集群上不存在的第三方 module， 我们可以先将整个目录归档压缩后上传到 HDFS 上，并使用 <code>-archives</code> 指定上传后的路径，即可在python 实现的 mapper 或 reducer 运行时引用 module 进行相关的计算。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">tar -czvf test_moudle.tar.gz -C test_moudle/ .</span><br><span class="line">hadoop fs -put test_moudle.tar.gz /tmp/cache/test_moudle.tar.gz</span><br><span class="line"># 打包上传完成后，在依赖未更新时，后续任务启动时不要重新打包上传归档文件</span><br><span class="line"># 在 mapper.py 或 reducer.py 直接用 ./tmodule 路径读取或者引用即可</span><br><span class="line"></span><br><span class="line">hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \</span><br><span class="line">  -archives hdfs://host:port/tmp/cache/test_moudle.tar.gz#tmodule \</span><br><span class="line">  -D mapreduce.job.reduces=1 \</span><br><span class="line">  -D mapreduce.job.name=&quot;TestArchives&quot; \</span><br><span class="line">  -input myInputDir \</span><br><span class="line">  -output myOutputDir \</span><br><span class="line">  -mapper &apos;python mapper.py&apos; \</span><br><span class="line">  -reducer &apos;python reducer.py&apos;</span><br></pre></td></tr></table></figure>

<h3 id="Streaming-任务本地测试"><a href="#Streaming-任务本地测试" class="headerlink" title="Streaming 任务本地测试"></a>Streaming 任务本地测试</h3><p>因为在集群上对失败任务进行 debug 比较麻烦一些，所以在提交任务之前，建议先在本地对任务进行简单的测试，测试通过后再提交到集群。  </p>
<p>简单测试命令如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat inputfile | sh mapper.sh | sort |sh reducer.sh &gt; output</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Hadoop-MapReduce</category>
      </categories>
      <tags>
        <tag>Hadoop Streaming</tag>
      </tags>
  </entry>
  <entry>
    <title>Yarn Shared Cache 功能简介</title>
    <url>/2020/01/20/yarn-shared-cache/</url>
    <content><![CDATA[<p>Yarn Shared Cache （Yarn 共享缓存服务）是社区 Hadoop 2.9 版本之后上新增的功能，参见<a href="https://issues.apache.org/jira/browse/YARN-1492" target="_blank" rel="noopener">YARN-1492</a>，它的目的是提供了一种安全且可扩展的方式向 HDFS 上传和管理应用程序依赖资源，降低 Yarn application 因为依赖资源的上传以及本地化带来的时间消耗。</p><a id="more"></a>
<p>通过使用 Yarn Shared Cache， 对于相同的依赖资源，YARN application 可以直接使用其他 application 上传的资源或者该 application 的先前运行时自己上传的资源，而无需每次都重新上传以及本地化相同的资源文件，从而节省网络资源并大大减少YARN应用程序启动时间。</p>
<p>本文主要简单介绍 Yarn Shared Cache 的架构和使用方式，具体官方文档可参见 <a href="https://hadoop.apache.org/docs/r2.9.2/hadoop-yarn/hadoop-yarn-site/SharedCache.html" target="_blank" rel="noopener">SharedCache</a></p>
<h2 id="架构"><a href="#架构" class="headerlink" title="架构"></a>架构</h2><p>Yarn Shared Cache 主要由4个部分组成:</p>
<ul>
<li><strong>Shared Cache Client: 共享缓存客户端</strong><br>主要提供与 SCM 交互的接口，用户或者开发者可以使用 shared cache client 计算资源校验值(checksum)，使用校验值向 SCM 获取共享缓存中该资源的存储路径。</li>
</ul>
<ul>
<li><p><strong>Shared Cache HDFS Directory：共享缓存 HDFS 存储目录</strong><br>共享存储资源全部都存储在指定的 HDFS 目录中，共享缓存目录通过HDFS权限进行保护，并且全局只读，只允许信任用户去写。这个目录只有共享缓存管理过程和在 NM 的 uploader 在上传资源时会修改。资源通过MD5 校验值被分到各个子目录。</p>
  <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">/sharedcache/a/8/9/a896857d078/foo.jar</span><br><span class="line">/sharedcache/5/0/f/50f11b09f87/bar.jar</span><br><span class="line">/sharedcache/a/6/7/a678cb1aa8f/job.jar</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>Shared Cache Manager (SCM)：共享缓存管理器</strong><br>SCM是一个单独的服务进程，它可以运行在任何节点上。这样，管理员可以在不影响其他组件的同时，启动，停止SCM。 </p>
<p>它负责处理从客户端以及 uploader 发来的请求，以及管理所有共享的资源。同时负责维护所有存储在 HDFS 目录中的缓存数据以及这些数据的元信息。它由两个主要的组件组成：后端存储服务和缓存清理服务。  </p>
<p>后端存储服务负责维护和持久化 shared cache 的元数据。包括在缓存中的资源，资源最后一次被使用时间，当前使用资源的应用等信息。目前是使用内存存储，并且在重启后会重建。  </p>
<p>清理服务负责确保不再使用的资源会被从缓存中移除。它周期性的扫描资源，清理过期的和没有应用使用的资源。 </p>
</li>
</ul>
<ul>
<li><p><strong>Shared Cache Uploader and Localization：缓存上传以及本地化</strong></p>
<p> Shared Cache uploader 是一个运行在  NM  上的服务，用来上传资源到共享缓存。它负责确认资源校验值，根据校验值动态确定资源对应的HDFS目录，然后上传资源到该目录中，然后通知 SCM 该资源已经被添加到缓存。</p>
<p> Shared Cache uploader 是异步的，不会阻塞 yarn应用的启动。</p>
<p> 一旦资源上传到共享缓存中，Yarn 使用 NM 本地化机制即可使得资源可获得。基于 NM 本地化资源的机制，由于 Shared Cache 中的缓存文件的 HDFS 地址和时间戳一直保持不变，当 NM 本地化过一次该资源后，后续再使用该资源时直接使用本地缓存的资源即可，也不会每次都重新本地化资源。因此，后续依赖于该资源的任务都不需要再次上传和本地化该资源，降低了任务启动时间。</p>
</li>
</ul>
<h2 id="部署"><a href="#部署" class="headerlink" title="部署"></a>部署</h2><h3 id="初始化设置"><a href="#初始化设置" class="headerlink" title="初始化设置"></a>初始化设置</h3><p>管理员需要通过以下几步来初始化设置 Shared Cache</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">1.创建HDFS目录，默认是 /sharecache 可配置</span><br><span class="line">    </span><br><span class="line">    yarn.sharedcache.root-dir=/sharecache </span><br><span class="line">    </span><br><span class="line">2.设置共享缓存目录0755</span><br><span class="line"></span><br><span class="line">    hadoop fs -chmod 755 /sharecache</span><br><span class="line"></span><br><span class="line">3.确认此目录的属主是 SCM 和 NM 的启动用户(yarn) </span><br><span class="line"></span><br><span class="line">    hadoop fs -chown yarn:yarn /sharecache</span><br><span class="line"></span><br><span class="line">4.在yarn-site.xml里面开启 sharedcache 功能</span><br><span class="line"></span><br><span class="line">    yarn.sharedcache.enabled=<span class="literal">true</span></span><br><span class="line">    </span><br><span class="line">5.启动共享缓存管理器</span><br><span class="line"></span><br><span class="line">    yarn --daemon start sharedcachemanager</span><br></pre></td></tr></table></figure>

<h3 id="所有配置选项"><a href="#所有配置选项" class="headerlink" title="所有配置选项"></a>所有配置选项</h3><p>Yarn Shared Cache 所需要的所有配置大致如下：</p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Default value</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr>
<td>yarn.sharedcache.enabled</td>
<td>false</td>
<td>是否开启 shared cache</td>
</tr>
<tr>
<td>yarn.sharedcache.root-dir</td>
<td>/sharedcache</td>
<td>shared cache 根目录</td>
</tr>
<tr>
<td>yarn.sharedcache.nested-level</td>
<td>3</td>
<td>非负整数，根据 checksum 子目录嵌套层数.</td>
</tr>
<tr>
<td>yarn.sharedcache.store.class</td>
<td>org.apache.hadoop.yarn.server.sharedcachemanager.store.InMemorySCMStore</td>
<td>SCM store 具体实现</td>
</tr>
<tr>
<td>yarn.sharedcache.app-checker.class</td>
<td>org.apache.hadoop.yarn.server.sharedcachemanager.RemoteAppChecker</td>
<td>SCM app-checker</td>
</tr>
<tr>
<td>yarn.sharedcache.store.in-memory.staleness-period-mins</td>
<td>10080</td>
<td>资源过期时间，如果资源上次被使用时间到现在超过过期时间则被认为是过期。单位是分钟</td>
</tr>
<tr>
<td>yarn.sharedcache.store.in-memory.initial-delay-mins</td>
<td>10</td>
<td>scm store 初始化延迟时间，单位分钟.</td>
</tr>
<tr>
<td>yarn.sharedcache.store.in-memory.check-period-mins</td>
<td>720</td>
<td>scm store application 检测周期，单位分钟.</td>
</tr>
<tr>
<td>yarn.sharedcache.admin.thread-count</td>
<td>1</td>
<td>SCM admin 接口处理线程数</td>
</tr>
<tr>
<td>yarn.sharedcache.cleaner.period-mins</td>
<td>1440</td>
<td>资源清理任务运行周期，单位分钟.</td>
</tr>
<tr>
<td>yarn.sharedcache.cleaner.initial-delay-mins</td>
<td>10</td>
<td>cleaner 任务初始化延迟时间，单位分钟.</td>
</tr>
<tr>
<td>yarn.sharedcache.cleaner.resource-sleep-ms</td>
<td>0</td>
<td>清理各个 resource 时的间隔时间. 单位 ms.</td>
</tr>
<tr>
<td>yarn.sharedcache.uploader.server.thread-count</td>
<td>50</td>
<td>SCM 处理 NM upload 请求的线程数</td>
</tr>
<tr>
<td>yarn.sharedcache.client-server.address</td>
<td>0.0.0.0:8045</td>
<td>The address of the client interface in the SCM</td>
</tr>
<tr>
<td>yarn.sharedcache.uploader.server.address</td>
<td>0.0.0.0:8046</td>
<td>The address of the node manager interface in the SCM</td>
</tr>
<tr>
<td>yarn.sharedcache.admin.address</td>
<td>0.0.0.0:8047</td>
<td>The address of the admin interface in the SCM</td>
</tr>
<tr>
<td>yarn.sharedcache.webapp.address</td>
<td>0.0.0.0:8788</td>
<td>The address of the web application in the SCM</td>
</tr>
<tr>
<td>yarn.sharedcache.client-server.thread-count</td>
<td>50</td>
<td>SCM 处理客户端请求的线程数</td>
</tr>
<tr>
<td>yarn.sharedcache.checksum.algo.impl</td>
<td>org.apache.hadoop.yarn.sharedcache.ChecksumSHA256Impl</td>
<td>计算文件 checksum 值算法</td>
</tr>
<tr>
<td>yarn.sharedcache.nm.uploader.replication.factor</td>
<td>10</td>
<td>resource 上传到 hdfs 后的备份数</td>
</tr>
<tr>
<td>yarn.sharedcache.nm.uploader.thread-count</td>
<td>20</td>
<td>NM 上传资源文件线程数</td>
</tr>
<tr>
<td>yarn.sharedcache.keytab</td>
<td>required for security cluster</td>
<td>keytab for SCM</td>
</tr>
<tr>
<td>yarn.sharedcache.principal</td>
<td>required for security cluster</td>
<td>principal for SCM</td>
</tr>
</tbody></table>
<h2 id="MR-支持-YARN-Shared-Cache"><a href="#MR-支持-YARN-Shared-Cache" class="headerlink" title="MR 支持 YARN Shared Cache"></a>MR 支持 YARN Shared Cache</h2><p>社区对 MR 任务进行了 YARN Shared Cache 功能支持，通过客户端配置来开启 MR 任务资源直接使用 Yarn Shared Cache进行托管。MR 客户端会优先使用 YARN Shared Cache 托管依赖资源，如果 SCM 服务不可用时，就会回退到原来的上传依赖资源到任务的提交目录的模式。 </p>
<p>使用主要使用的配置如下。</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>Whether the shared cache is enabled<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.sharedcache.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"> </span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>The address of the client interface in the SCM (shared cache manager)<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.sharedcache.client-server.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>shardcache-server-host:8045<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"> </span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.sharedcache.keytab<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>/etc/hadoop/conf/yarn.keytab<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.sharedcache.principal<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn/_HOST@HADOOP.COM<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.job.sharedcache.mode<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">value</span>&gt;</span>libjars,files,archives<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">description</span>&gt;</span></span><br><span class="line">         A comma delimited list of resource categories to submit to the</span><br><span class="line">         shared cache. The valid categories are: jobjar, libjars, files,</span><br><span class="line">         archives. If "disabled" is specified then the job submission code</span><br><span class="line">         will not use the shared cache.</span><br><span class="line">      <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>




<h2 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h2><ul>
<li><p><a href="https://hadoop.apache.org/docs/r2.9.2/hadoop-yarn/hadoop-yarn-site/SharedCache.html" target="_blank" rel="noopener">SharedCache</a></p>
</li>
<li><p><a href="https://hadoop.apache.org/docs/r3.1.2/hadoop-mapreduce-client/hadoop-mapreduce-client-core/SharedCacheSupport.html" target="_blank" rel="noopener">MR Support for YARN Shared Cache</a></p>
</li>
</ul>
]]></content>
      <categories>
        <category>Hadoop-YARN</category>
      </categories>
      <tags>
        <tag>Yarn</tag>
        <tag>Shared Cache</tag>
      </tags>
  </entry>
  <entry>
    <title>MapReduce 任务提交加速</title>
    <url>/2019/12/20/mr-job-submit-optimization/</url>
    <content><![CDATA[<p>在日常工作中，我们经常遇到一些用户反馈任务提交特别慢，有些任务从运行启动脚本到任务真正提交到 RM 上，需要十几分钟有的甚至需要一个小时以上。分析这些任务我们很容易就发现，这些任务基本上都是因为依赖的本地资源(files, libjars, archives)文件过多或者输入数据过多(split input巨慢)导致的，因此本篇文章主要就是简单分析一下 MR 任务提交过程，然后总结一下加快提交速度的方法。</p><a id="more"></a>
<h2 id="MR-任务提交过程"><a href="#MR-任务提交过程" class="headerlink" title="MR 任务提交过程"></a>MR 任务提交过程</h2><p>MRv2 中，任务的提交过程的代码调用路径大致是<br><code>org.apache.hadoop.mapreduce.Job#submit --&gt; org.apache.hadoop.mapreduce.JobSubmitter#submitJobInternal --&gt; org.apache.hadoop.mapred.YARNRunner#submitJob</code>.</p>
<p>代码的逻辑也比较简单，主要做了以下几件事情：</p>
<ol>
<li><p><strong>创建 submitDir，上传依赖资源 (files, libjars, archives):</strong><br>这部分主要负责将任务在集群上运行时需要的资源文件上传到任务在 HDFS 上的 submitDir，以便于任务在 NM 上启动时，可以从 submitDir 将依赖资源本地化到 NM 上的任务的本地目录中，从而使任务运行时直接可以使用这些依赖资源。  </p>
<p>因为涉及到写入数据到 HDFS，所以当任务有许多的依赖资源需要上传时，该过程就会变得十分耗时。比如一些机器学习任务，会依赖特定版本的 python，以及一堆的 models 或者框架，上传过程需要将所有的依赖的文件都上传到 summitDir，因此耗时巨长。</p>
</li>
<li><p><strong>输入数据分片(input split):</strong><br>这部分主要是遍历任务的输入目录，将所有输入数据文件按照分片规则进行分片操作，分片完成后将分片结果写入 submitDir，然后根据分片数量确定 map 数量。  </p>
<p>因为涉及到对于 HDFS 输入路径的遍历访问，所以当输入数据很多时，该过程也会变得非常耗时。</p>
</li>
<li><p><strong>提交任务到 RM：</strong><br>这部分工作就比较简单，主要就是根据客户端配置生成任务提交请求，然后向 RM 发起任务提交。这部分工作比较简单，一般不存在比较耗时的状况。</p>
</li>
</ol>
<h2 id="提交任务加速"><a href="#提交任务加速" class="headerlink" title="提交任务加速"></a>提交任务加速</h2><p>根据任务提交流程分析，我们知道了任务提交慢的原因主要是依赖资源过多或者输入数据过多导致。针对这两种情况，我们可以给出对应的解决方法。</p>
<h3 id="依赖资源过多"><a href="#依赖资源过多" class="headerlink" title="依赖资源过多"></a>依赖资源过多</h3><p>对于依赖资源过多的问题，优化方法主要有三种。</p>
<ol>
<li><p><strong>本地依赖资源统一压缩打包后上传</strong><br>因为依赖资源文件上传 HDFS 是单个文件逐一上传的，依赖文件过多时，就会有许多次文件新建、写入关闭、过程，其中文件的新建和关闭耗费的时间可能比实际数据写入耗时还多。因此，我们可以考虑将所有依赖资源使用 tar 命令压缩打包成一个 tar 包，使用 <code>-archives source.tar.gz</code> 进行上传，从而达到减少文件创建过程，只有文件上传的耗时。</p>
<p>这种方式会降低一大部分耗时，但是也会带来文件打包的时间消耗，总体耗时降低可能并不太明显。并且NM 本地化时会将 tar 包自动解压成为目录，需要将代码里所有将依赖路径改成 <code>source/path/xxx</code> 这种。因此这种方式只适合任务的依赖资源每次都会变化的任务，对于依赖资源基本不变的任务，可以考虑使用方法2.</p>
</li>
</ol>
<ol start="2">
<li><p><strong>依赖资源预先上传到 HDFS</strong><br>查看资源上传代码我们发现，任务上传依赖资源时，会检测依赖资源路径是不是 HDFS 路径，如果是就不再上传该资源。所以当我们预先把所有依赖文件预先上传到 HDFS 指定目录，然后在指定依赖文件时直接使用 HDFS 路径 (<code>-archives hdfs://ns1/tmp/archives</code>)时，任务提交时就会跳过该资源上传流程。</p>
<p>这种方式可以说是一次上传，永远使用。后续任务提交时依赖资源上传流程几乎不存在耗时，能够大大减少依赖资源巨多的任务的提交时间。该方式适合依赖资源基本不变的任务，依赖资源上传后不需要修改。</p>
</li>
<li><p><strong>使用 yarn shared cache</strong><br>yarn shared cache 是社区在 2.9 版本后提出的新功能，主要思想是使用单独的服务来托管所有集群任务的依赖资源。对于相同的依赖资源，直接从 shared cache 获取，不需要每次都上传。具体可参见 <a href="yarn-shared-cache">YARN Shared Cache 功能简介</a></p>
</li>
</ol>
<h3 id="输入数据过多"><a href="#输入数据过多" class="headerlink" title="输入数据过多"></a>输入数据过多</h3><p>输入数据过多会导致任务提交过程中需要花费大量时间来遍历输入数据，然后对输入数据进行分片。</p>
<p><code>FileInputFormat</code> 默认使用单线程遍历输入数据，但是提供了多线程处理功能，通过使用配置设置处理<br>线程数量即可。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">mapreduce.input.fileinputformat.list-status.num-threads=<span class="number">10</span></span><br></pre></td></tr></table></figure>
<p>经过实践，通过启用多线程处理功能，输入数据处理过程时间会成倍数的降低。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>如上述分析，MR 任务提交慢的的原因，要么是依赖资源过多导致上传耗时长，要么是输入数据过多导致输入分片耗时长。所以针对具体任务，我们分析下任务的具体状态，再根据上述的优化方法进行针对话优化即可。</p>
]]></content>
      <categories>
        <category>Hadoop-MapReduce</category>
      </categories>
      <tags>
        <tag>MapReduce</tag>
        <tag>性能调优</tag>
      </tags>
  </entry>
  <entry>
    <title>YARN 在 security 集群中使用统一用户运行 Container</title>
    <url>/2019/11/20/yarn-limit-user-in-secure-mode/</url>
    <content><![CDATA[<p>在我们的工作过程中，随着业务越来越多，Hadoop 集群规模也越来越大，同时对集群的安全性也越来越高，最原始的 SIMPLE 验证方式已经无法满足需求了，因此，我们决定使用 kerberos, 开启集群的安全验证: Run Hadoop in Secure Mode。</p><p>在实践中过程中，我们发现在 Secure Mode 时，NM 必须使用 LinuxContainerExecutor，如果使用 DefaultContainerExecutor 的话，MR 任务会在 reduce shuffle 阶段失败退出，参见<a href="https://issues.apache.org/jira/browse/YARN-1432" target="_blank" rel="noopener">YARN-1432</a>。同时，LCE 会使用任务提交用户 JobUser 来运行 Container，如果 NM 上不存在 JobUser，任务就会失败退出。 </p><a id="more"></a>

<p>其实简单想想就能理解 Hadoop 这样设计的逻辑是为了实现用户隔离，保证用户任务和数据的安全，但是在实践过程中，由于我们集群中用户过多，在 NM 上逐一添加用户过于繁琐，而且后续维护起来比较麻烦，因此，我们决定修改这块的代码逻辑，使 NM 在 Secure Mode 下支持使用统一的 yarn 用户来执行任务。</p>
<h2 id="原始逻辑解析"><a href="#原始逻辑解析" class="headerlink" title="原始逻辑解析"></a>原始逻辑解析</h2><p>为了实现我们的目的，先得了解 NM 这块代码的实现逻辑，所以我对这块的代码逻辑进行了梳理，发现安全用户限制的逻辑存在于 LinuxContainerExecutor 和 SecureIOUtils 两个类中。</p>
<h3 id="LinuxContainerExecutor"><a href="#LinuxContainerExecutor" class="headerlink" title="LinuxContainerExecutor"></a>LinuxContainerExecutor</h3><p>LCE 在初始化，启动以及清理 container 之前，都会调用 <code>getRunAsUser(jobUser)</code> 方法获取该 container 实际运行时该使用的用户，再使用该用户去对 container 执行相应的操作。</p>
<p>如果集群未开启安全验证方式（使用 SIMPLE 验证方式： 如果配置 nonsecure 模式下限制用户时，getRunAsUser 返回统一的用户，默认是 nobody。如果配置不限制用户时，getRunAsUser 返回任务提交用户。</p>
<p>如果集群开启了安全验证方式 (使用 kerberos 验证方式)： getRunAsUser 返回任务提交用户。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Determine if UserGroupInformation is using Kerberos to determine</span></span><br><span class="line"><span class="comment"> * user identities or is relying on simple authentication</span></span><br><span class="line"><span class="comment"> * </span></span><br><span class="line"><span class="comment"> * <span class="doctag">@return</span> true if UGI is working in a secure environment</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">boolean</span> <span class="title">isSecurityEnabled</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  <span class="keyword">return</span> !isAuthenticationMethodEnabled(AuthenticationMethod.SIMPLE);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// nonsecure 模式下是否限制用户，默认 true</span></span><br><span class="line"><span class="keyword">boolean</span> containerLimitUsers = conf.getBoolean(</span><br><span class="line">  <span class="string">"yarn.nodemanager.linux-container-executor.nonsecure-mode.limit-users"</span>, <span class="keyword">true</span>);</span><br><span class="line"><span class="comment">// nonsecure 模式下默认使用的本地用户， 默认 nobody</span></span><br><span class="line">String nonsecureLocalUser = conf.get(</span><br><span class="line">  <span class="string">"yarn.nodemanager.linux-container-executor.nonsecure-mode.local-user"</span> ,<span class="string">"nobody"</span>);</span><br><span class="line"></span><br><span class="line"><span class="function">String <span class="title">getRunAsUser</span><span class="params">(String user)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">if</span> (UserGroupInformation.isSecurityEnabled() ||</span><br><span class="line">      !containerLimitUsers) &#123;</span><br><span class="line">    <span class="keyword">return</span> user;</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="keyword">return</span> nonsecureLocalUser;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>LCE 获取到实际用户后，会去调用 container-executor 二进制可执行文件来初始化和启动 container，container-executor 在启动时会执行 set_user 函数来设置 container 实际的执行用户，在这个函数中又会调用 check_user 函数，对该用户进行一系列的校验，主要包括：不是 root， UID 大于配置的 minUid，该用户本地实际存在以及不在禁用用户列表里。当校验不通过时，就会报错退出。所以，当 LCE 传递进来的指定用户在机器上不存在时，container 就会失败退出。</p>
<p>container-executor.c</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Is the user a real user account?</span></span><br><span class="line"><span class="comment"> * Checks:</span></span><br><span class="line"><span class="comment"> *   1. Not root</span></span><br><span class="line"><span class="comment"> *   2. UID is above the minimum configured.</span></span><br><span class="line"><span class="comment"> *   3. Not in banned user list</span></span><br><span class="line"><span class="comment"> * Returns NULL on failure</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function">struct passwd* <span class="title">check_user</span><span class="params">(<span class="keyword">const</span> <span class="keyword">char</span> *user)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">if</span> (<span class="built_in">strcmp</span>(user, <span class="string">"root"</span>) == <span class="number">0</span>) &#123;</span><br><span class="line">    <span class="built_in">fprintf</span>(LOGFILE, <span class="string">"Running as root is not allowed\n"</span>);</span><br><span class="line">    fflush(LOGFILE);</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">NULL</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">char</span> *min_uid_str = get_section_value(MIN_USERID_KEY, &amp;executor_cfg);</span><br><span class="line">  <span class="keyword">int</span> min_uid = DEFAULT_MIN_USERID;</span><br><span class="line">  <span class="keyword">if</span> (min_uid_str != <span class="literal">NULL</span>) &#123;</span><br><span class="line">    <span class="keyword">char</span> *end_ptr = <span class="literal">NULL</span>;</span><br><span class="line">    min_uid = strtol(min_uid_str, &amp;end_ptr, <span class="number">10</span>);</span><br><span class="line">    <span class="keyword">if</span> (min_uid_str == end_ptr || *end_ptr != <span class="string">'\0'</span>) &#123;</span><br><span class="line">      <span class="built_in">fprintf</span>(LOGFILE, <span class="string">"Illegal value of %s for %s in configuration\n"</span>,</span><br><span class="line">	      min_uid_str, MIN_USERID_KEY);</span><br><span class="line">      fflush(LOGFILE);</span><br><span class="line">      <span class="built_in">free</span>(min_uid_str);</span><br><span class="line">      <span class="keyword">return</span> <span class="literal">NULL</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">free</span>(min_uid_str);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="class"><span class="keyword">struct</span> <span class="title">passwd</span> *<span class="title">user_info</span> = <span class="title">get_user_info</span>(<span class="title">user</span>);</span></span><br><span class="line">  <span class="keyword">if</span> (<span class="literal">NULL</span> == user_info) &#123;</span><br><span class="line">    <span class="built_in">fprintf</span>(LOGFILE, <span class="string">"User %s not found\n"</span>, user);</span><br><span class="line">    fflush(LOGFILE);</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">NULL</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">if</span> (user_info-&gt;pw_uid &lt; min_uid &amp;&amp; !is_whitelisted(user)) &#123;</span><br><span class="line">    <span class="built_in">fprintf</span>(LOGFILE, <span class="string">"Requested user %s is not whitelisted and has id %d,"</span></span><br><span class="line">	    <span class="string">"which is below the minimum allowed %d\n"</span>, user, user_info-&gt;pw_uid, min_uid);</span><br><span class="line">    fflush(LOGFILE);</span><br><span class="line">    <span class="built_in">free</span>(user_info);</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">NULL</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">char</span> **banned_users = get_section_values(BANNED_USERS_KEY, &amp;executor_cfg);</span><br><span class="line">  banned_users = banned_users == <span class="literal">NULL</span> ?</span><br><span class="line">    (<span class="keyword">char</span>**) DEFAULT_BANNED_USERS : banned_users;</span><br><span class="line">  <span class="keyword">char</span> **banned_user = banned_users;</span><br><span class="line">  <span class="keyword">for</span>(; *banned_user; ++banned_user) &#123;</span><br><span class="line">    <span class="keyword">if</span> (<span class="built_in">strcmp</span>(*banned_user, user) == <span class="number">0</span>) &#123;</span><br><span class="line">      <span class="built_in">free</span>(user_info);</span><br><span class="line">      <span class="keyword">if</span> (banned_users != (<span class="keyword">char</span>**)DEFAULT_BANNED_USERS) &#123;</span><br><span class="line">        free_values(banned_users);</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="built_in">fprintf</span>(LOGFILE, <span class="string">"Requested user %s is banned\n"</span>, user);</span><br><span class="line">      <span class="keyword">return</span> <span class="literal">NULL</span>;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">if</span> (banned_users != <span class="literal">NULL</span> &amp;&amp; banned_users != (<span class="keyword">char</span>**)DEFAULT_BANNED_USERS) &#123;</span><br><span class="line">    free_values(banned_users);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> user_info;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="SecureIOUtils"><a href="#SecureIOUtils" class="headerlink" title="SecureIOUtils"></a>SecureIOUtils</h3><p>SecureIOUtils 是一个公用的提供安全的读写本地文件接口的工具类，当集群开启安全验证之后，SecureIOUtils 提供的读写接口会对发起读写用户与本地文件的属主进行对比，如果不一致，则抛出 IOException。</p>
<p>MR 的 ShuffleHandler 在读取 map 输出的 spill 文件时会调用 SecureIOUtils，ShuffleHandler 读时使用的用户任务提交用户，当用户与文件属主不一致时，shuffle 就会一直失败，最终任务的 reduce 因为 shuffle 失败而退出。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">checkStat</span><span class="params">(File f, String owner, String group, </span></span></span><br><span class="line"><span class="function"><span class="params">    String expectedOwner, </span></span></span><br><span class="line"><span class="function"><span class="params">    String expectedGroup)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">  <span class="keyword">boolean</span> success = <span class="keyword">true</span>;</span><br><span class="line">  <span class="keyword">if</span> (expectedOwner != <span class="keyword">null</span> &amp;&amp;</span><br><span class="line">      !expectedOwner.equals(owner)) &#123;</span><br><span class="line">    <span class="keyword">if</span> (Path.WINDOWS) &#123;</span><br><span class="line">      UserGroupInformation ugi =</span><br><span class="line">          UserGroupInformation.createRemoteUser(expectedOwner);</span><br><span class="line">      <span class="keyword">final</span> String adminsGroupString = <span class="string">"Administrators"</span>;</span><br><span class="line">      success = owner.equals(adminsGroupString)</span><br><span class="line">          &amp;&amp; ugi.getGroups().contains(adminsGroupString);</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      success = <span class="keyword">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">if</span> (!success) &#123;</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> IOException(</span><br><span class="line">        <span class="string">"Owner '"</span> + owner + <span class="string">"' for path "</span> + f + <span class="string">" did not match "</span> +</span><br><span class="line">            <span class="string">"expected owner '"</span> + expectedOwner + <span class="string">"'"</span>);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="优化方案"><a href="#优化方案" class="headerlink" title="优化方案"></a>优化方案</h2><p>根据上述的代码逻辑分析，我们发现其实想要实现我们的目的其实是很简单的，只要修改两点就可以做到：</p>
<ul>
<li>LCE getRunAsUser 支持 security 模式下返回统一用户</li>
<li>SecureIOUtils 支持 security 模式下用户读取不属于自己的文件</li>
</ul>
<p>代码修改实现起来也很简单，但是我们还是需要考虑一下怎样实现更加优雅，能更好的兼容原有的功能。说到这，额外多说一句，在日常的项目开发工作中，有很多朋友都喜欢以实现功能优先，不考虑其他的问题，先使用最快最简单的方式实现。比如说上述两个功能，很多朋友可能就直接修改 LCE 的 getRunAsUser 方法使之在任何情况下都返回同一个基于配置用户，更有甚者直接硬编码返回 <code>yarn</code> 用户，然后 SecureIOUtils 中直接去除安全模式时的用户比较。这样一来基本没什么修改就实现了功能。</p>
<p>对于这种开发方式，其实我是不太赞同的，因为这种开发方式确实能够快速满足当前需求，但是往往都是以牺牲掉代码的兼容性和扩展性为代价的。这种方式，对于一些比较小的项目，可能还能够适用，对于类似 hadoop 这种大型的持续开发演进的系统，这种方式就是不可取的。比如我刚才举例的实现方式，确实快，可能也就改了 2 行代码，但是它的后果就是，修改上线后的 NM 不再支持用户权限隔离功能了，后续如果业务真有了这个需求，也是没办法支持的，除非把代码再改回来。</p>
<p>因此，在 hadoop 这种大型系统的功能开发工作中，我们在实现功能的同时，更要考虑实现功能的方式与系统本身的兼容性，以及功能后续的扩展性，以防止当前的实现方式在后续带来更多的问题。具体到本文中的功能，我们首先应该考虑在保证在 security 模式下 NM 原有的任务用户隔离功能的基础上，同时支持 NM 使用同一用户来运行所有 container 。我们都知道 hadoop 基于配置的，很多功能属性的启用都是通过配置，所以我们在实现该功能时，也应该通过配置来实现。具体配置如下：</p>
<p>YarnConfiguration.java</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">// secure-mode 模式下是否开启用户限制，默认为 true, 即使用同一用户运行 container</span></span><br><span class="line"><span class="comment">// 设置成为 false 时，则与原始逻辑一致 </span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> String NM_SECURE_MODE_LIMIT_USERS = NM_PREFIX +</span><br><span class="line">    <span class="string">"linux-container-executor.secure-mode.limit-users"</span>;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">boolean</span> DEFAULT_NM_SECURE_MODE_LIMIT_USERS = <span class="keyword">true</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// secure-mode 模式下开启用户限制时使用的用户，默认 nobody，建议配置为 yarn</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> String NM_SECURE_MODE_LOCAL_USER_KEY = NM_PREFIX +</span><br><span class="line">    <span class="string">"linux-container-executor.secure-mode.local-user"</span>;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> String DEFAULT_NM_SECURE_MODE_LOCAL_USER = <span class="string">"nobody"</span>;</span><br></pre></td></tr></table></figure>
<p>CommonConfigurationKeys.java</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">// secure-mode 模式下是否限制用户只能读写属于自己的本地文件，默认是 true，与原始逻辑一致</span></span><br><span class="line"><span class="comment">// 设置成 false 时，会跳过文件用户检测，及允许用户读取不属于自己的本地文件。</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> String HADOOP_SECURITY_IO_LIMIT_USER_KEY =</span><br><span class="line">    <span class="string">"hadoop.security.io.limit-user"</span>;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">boolean</span> HADOOP_SECURITY_IO_LIMIT_USER_DEFAULT =<span class="keyword">true</span>;</span><br></pre></td></tr></table></figure>

<p>具体代码就不全部贴出来了，参见基于 3.2.0 分支实现的 <a href="yarn_limit_user_in_securemode-branch-3.2.0.patch">branch-3.2.0.patch</a> 即可。patch 合并之后，通过设置以下配置我们就可以实现在 security 模式下，NM 统一使用 yarn 用户来运行任务。</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.linux-container-executor.secure-mode.limit-users<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.linux-container-executor.secure-mode.local-user<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.security.io.limit-user<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>Hadoop-YARN</category>
      </categories>
      <tags>
        <tag>Yarn</tag>
        <tag>LinuxContainerExecutor</tag>
        <tag>Security</tag>
      </tags>
  </entry>
  <entry>
    <title>HDFS 问题总结</title>
    <url>/2019/11/01/hdfs-problems-collection/</url>
    <content><![CDATA[<p>在日常维护 HDFS 集群以及为用户提供技术支持时，我们经常会遇到各种 HDFS 相关异常问题，对于一些常见的问题，我们可能知道如何去解决，所以处理起来会比较快。但是当遇到一些不常见的问题时，我们可能没办法根据异常信息直接找到解决方法，可能需要查找一些资料，或者深入源码层面查找问题所在，处理起来比较费时费力，很可能对集群的业务造成影响。</p><a id="more"></a>
<p>因此，将实际生产环境中 HDFS 集群出现的问题进行总结是有必要的。在本文中，我会把我在集群运维过程遇到的一些有代表性的问题进行总结，包括异常信息，产生原因以及解决办法，以便为后续的运维工作提供一个快速参照，也希望能给大家带来一定的参考。</p>
<h2 id="1-文件未正常关闭"><a href="#1-文件未正常关闭" class="headerlink" title="1. 文件未正常关闭"></a>1. 文件未正常关闭</h2><h3 id="异常信息"><a href="#异常信息" class="headerlink" title="异常信息"></a>异常信息</h3><p>主要异常信息有以下几种：</p>
<ul>
<li><p>Cannot obtain block length<br>出现在读取未正常关闭文件时</p>
</li>
<li><p>Mismatch in length of source<br>出现在 distcp 文件后，对比源文件与复制文件长度时</p>
</li>
</ul>
<h3 id="产生原因"><a href="#产生原因" class="headerlink" title="产生原因"></a>产生原因</h3><p>正常 HDFS 客户端向 HDFS 写入数据完成时，会向 NameNode 发起 <code>completeFile</code> 的操作，NN 端会 <code>commit &amp; complete</code> 文件的最后一个 block，然后更新文件 INode 信息，主要是加入 last block 长度， 再 <code>finalize INodeFile</code> 关闭文件，释放文件租约。此时文件对于客户端来说，就是正常关闭的完整文件。</p>
<p>但是如果写入数据时发生了异常，比如客户端崩溃或者 namenode 异常等，completeFile 未成功发起或者完成，就会导致文件最后一个 block 可能未正常关闭，INodeFile 记录的文件长度与实际写入的长度不匹配，租约也无法正常释放。此时文件对于客户端来讲，是未关闭的异常状态，也就是 <code>openforwrite</code> 的状态，此时去读文件时，就会出现上述的异常状态。</p>
<h3 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h3><p>这种情况的解决方案是先找出未正常关闭的文件，然后通过手动释放租约，关闭文件。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">1. 使用 fsck 找出未正常关闭文件</span><br><span class="line">hdfs fsck path -openforwrite </span><br><span class="line"></span><br><span class="line">对于 federation 集群需要指定 defaultFS</span><br><span class="line">hdfs fsck -Dfs.defaultFS=hdfs://nameservice:8020/ path -openforwrite </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">2. 手动释放租约</span><br><span class="line">hdfs debug recoverLease -path openfile -retries 3</span><br></pre></td></tr></table></figure>

<h2 id="2-安全集群-kerberos-无法访问非安全集群"><a href="#2-安全集群-kerberos-无法访问非安全集群" class="headerlink" title="2. 安全集群 (kerberos) 无法访问非安全集群"></a>2. 安全集群 (kerberos) 无法访问非安全集群</h2><h3 id="异常信息-1"><a href="#异常信息-1" class="headerlink" title="异常信息"></a>异常信息</h3><p>HDFS 支持使用 kerberos 认证作为用户权限校验工具，开启的集群的 security 模式。 安全模式的配置大致为</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.security.authentication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>kerberos<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.security.authorization<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>当 security 集群 客户端通过指定 namenode 地址访问另一个 non security 的的集群时会有以下报错：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">hadoop fs -ls hdfs:master001.mars.sjs.ted:<span class="number">8020</span>/user </span><br><span class="line"></span><br><span class="line"><span class="number">19</span>/<span class="number">05</span>/<span class="number">31</span> <span class="number">17</span>:<span class="number">14</span>:<span class="number">34</span> INFO configfile.ConfigFileHasClientPlugin: Get the user info successfully, login user : hdfs</span><br><span class="line">ls: Failed on local exception: java.io.IOException: Server asks us to fall back to SIMPLE auth, but <span class="keyword">this</span> client is configured to only allow secure connections.; Host Details : local host is: <span class="string">"cloud_dev.gd.ted/10.135.43.214"</span>; destination host is: <span class="string">"master001.mars.sjs.ted"</span>:<span class="number">8020</span>;</span><br></pre></td></tr></table></figure>

<h3 id="产生原因-1"><a href="#产生原因-1" class="headerlink" title="产生原因"></a>产生原因</h3><p>当客户端开启了安全认证模式，而请求的 service 服务使用 SIMPLE 模式，未开启安全认证的时候 IPC Client 在与 server 端建立连接时，会因为验证方式不一致而抛出此异常。</p>
<h3 id="解决方式"><a href="#解决方式" class="headerlink" title="解决方式"></a>解决方式</h3><p>解决方法是在客户端添加配置，允许 security 的客户端连接 non security &amp; simple 模式的服务端建立连接。</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span>   </span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>ipc.client.fallback-to-simple-auth-allowed<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h2 id="3-datanode-域名无法解析"><a href="#3-datanode-域名无法解析" class="headerlink" title="3. datanode 域名无法解析"></a>3. datanode 域名无法解析</h2><h3 id="异常信息-2"><a href="#异常信息-2" class="headerlink" title="异常信息"></a>异常信息</h3><p>在客户端读写 HDFS 数据的时候，有时会出现以下的异常</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">java.nio.channels.UnresolvedAddressException</span><br><span class="line">	at sun.nio.ch.Net.checkAddress(Net.java:<span class="number">101</span>)</span><br><span class="line">	at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:<span class="number">622</span>)</span><br><span class="line">	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:<span class="number">192</span>)</span><br><span class="line">	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:<span class="number">530</span>)</span><br></pre></td></tr></table></figure>

<h3 id="产生原因-2"><a href="#产生原因-2" class="headerlink" title="产生原因"></a>产生原因</h3><p>当客户端向 HDFS 发起读写请求时，NameNode 会根据目标文件的 block 存储位置返回一组相应的 datanode 节点给客户端，默认状况下，客户端根据返回的 datanode 的域名和端口信息，发起 socket 连接，进行数据读写。</p>
<p>在集群维护过程中，我们发现有些小型集群，为了省事，未在 dns 服务器上配置可解析的域名，而是所有 datanode 节点本地配置域名，同时在所有 datanode 节点上的 /etc/hosts 里面添加映射来完成域名解析。这种情况下，客户端在读写集群时，根据 datanode 域名去解析 ip 时，就会有这种无法解析域名 UnresolvedAddressException 的报错。</p>
<h3 id="解决方法-1"><a href="#解决方法-1" class="headerlink" title="解决方法"></a>解决方法</h3><p>这种状况的解决方式有两种：  </p>
<p>首先第一种就是跟集群上的 datanode 节点一样，将所有的节点域名映射添加到 /etc/hosts 里，这种方式比较笨，而且难以维护，必须要跟这集群节点信息的更新而更新，所以不建议这样解决。</p>
<p>第二种方式是将客户端禁止默认使用域名进行连接，这样客户端就会默认使用节点 ip 进行连接，也就跳过了域名解析的问题。客户端只需要在配置 <code>dfs.client.use.datanode.hostname=false</code> 即可，且不需要频繁更改，所以推荐使用这种解决方式。</p>
<h2 id="4-datanode-注册失败，机架信息异常"><a href="#4-datanode-注册失败，机架信息异常" class="headerlink" title="4. datanode 注册失败，机架信息异常"></a>4. datanode 注册失败，机架信息异常</h2><h3 id="异常信息-3"><a href="#异常信息-3" class="headerlink" title="异常信息"></a>异常信息</h3><p>当集群新增 datanode 节点，启动时无法向 NN 成功注册，异常信息如下</p>
<p>NN 端 异常：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">org.apache.hadoop.net.NetworkTopology: Error: can<span class="string">'t add leaf node /J/HD-1100/10.140.49.36:50010 at depth 3 to topology:</span></span><br><span class="line"><span class="string">org.apache.hadoop.net.NetworkTopology$InvalidTopologyException: Failed to add /J/HD-1164/10.140.83.70:50010: You cannot have a rack and a non-rack node at the same level of the network topology.</span></span><br><span class="line"><span class="string">	at org.apache.hadoop.net.NetworkTopology.add(NetworkTopology.java:415)</span></span><br><span class="line"><span class="string">	at org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager.registerDatanode(DatanodeManager.java:980)</span></span><br><span class="line"><span class="string">    ...</span></span><br></pre></td></tr></table></figure>

<p>DN 端 异常：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.net.NetworkTopology$InvalidTopologyException): Failed to add /J/HD-<span class="number">1164</span>/<span class="number">10.140</span>.83.70:<span class="number">50010</span>: You cannot have a rack and a non-rack node at the same level of the network topology.</span><br><span class="line">        at org.apache.hadoop.net.NetworkTopology.add(NetworkTopology.java:<span class="number">415</span>)</span><br><span class="line">        at org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager.registerDatanode(DatanodeManager.java:<span class="number">980</span>)</span><br><span class="line">        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.registerDatanode(FSNamesystem.java:<span class="number">5218</span>)</span><br><span class="line">        ...</span><br></pre></td></tr></table></figure>

<h3 id="产生原因-3"><a href="#产生原因-3" class="headerlink" title="产生原因"></a>产生原因</h3><p>datanode 节点的机架信息代表着 datanode 在集群的网络拓扑结构中所在的位置， NN 通过机架信息来计算节点之间的距离，进而通过选用距离最近的节点，提升集群读写任务的本地性，达到读写加速的效果。</p>
<p>机架信息通常使用 /d/r/n 这种3层机构来标识， d 为 data center ，表示数据中心， r 为rack， 表示机架信息， n 为 node，标识节点信息。</p>
<p>通过追踪 datanode 注册模块的代码，我们发现该异常产生原因是 Namenode 通过配置 <code>net.topology.table.file.name</code> 指定的节点机架信息配置文件来维护所有节点的机架信息。如果配置文件中不存在某个节点的机架信息，该节点的机架信息就为默认的 <code>/default-rack</code> ， NN 使用机架信息中的 ‘/‘ 数量来标识机架信息的深度。<strong>最为重要的，一个集群只允许有一种深度的机架信息，所以新增的 datanode 的机架深度必须跟已有的机架信息深度完全一致。</strong> </p>
<p>所以，当新增节点的机架信息与原来节点机架信息深度不一致时，就会出现此异常。具体可能是新增节点未配置机架信息，使用的 <code>/default-rack</code> 与原来节点不一致，也可能是配置时，配置的机架深度与原来节点不一致。</p>
<h3 id="解决方式-1"><a href="#解决方式-1" class="headerlink" title="解决方式"></a>解决方式</h3><p>检查 <code>net.topology.table.file.name</code> 指定的机架信息配置文件中，新节点的机架信息是否配置了，如果未配置，按照之前的规则添加正确的机架信息，如果配置了，检查是否正确，如果不正确，修正即可。如果是正确的，可能要考虑之前节点的机架信息是否配置上了，很有可能之前的机架信息没有配置正确，用的都是 <code>/default-rack</code></p>
<h2 id="5-Federation-模式下-Datanode-只服务一组-NameNode"><a href="#5-Federation-模式下-Datanode-只服务一组-NameNode" class="headerlink" title="5. Federation 模式下 Datanode 只服务一组 NameNode"></a>5. Federation 模式下 Datanode 只服务一组 NameNode</h2><h3 id="异常信息-4"><a href="#异常信息-4" class="headerlink" title="异常信息"></a>异常信息</h3><p>异常出现在搭建 federation 集群时， format &amp; start 多组 NameNode 后，启动 Datanode 时，发现 Datanode 向多组 NameNode 发起注册请求时，只有一组 NameNode 能够注册成功，其余的注册汇报以下异常：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">2019-12-18 18:40:58,192 FATAL org.apache.hadoop.hdfs.server.datanode.DataNode: Initialization failed for Block pool &lt;registering&gt; (Datanode Uuid unassigned) service to master02.highschool.sjs.ted/10.144.104.117</span><br><span class="line">:8020. Exiting. </span><br><span class="line">java.io.IOException: Cluster IDs not matched: dn cid=CID-72c0bfd0-3550-4eb2-97ce-90e46723ddcf but ns cid=CID-232e4ad4-c76d-46a7-b89f-afe8324045cd; bpid=BP-144350400-10.144.104.101-1576662236699</span><br><span class="line">        at org.apache.hadoop.hdfs.server.datanode.DataNode.setClusterId(DataNode.java:717)</span><br><span class="line">        at org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:1383)</span><br><span class="line">        at org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:315)</span><br><span class="line">        at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:219)</span><br><span class="line">        at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:673)</span><br><span class="line">        at java.lang.Thread.run(Thread.java:748)</span><br></pre></td></tr></table></figure>

<h3 id="产生原因-4"><a href="#产生原因-4" class="headerlink" title="产生原因"></a>产生原因</h3><p>federation 模式下,<em>所有 NameService 需要保持一个统一的 clusterid</em>，来表明它们属于同一个集群。 clusterid 由格式化 NameNode 时，使用 <code>hdfs namenode -format -clusterid id</code> 来指定，如果未指定，直接使用 <code>hdfs namenode -format</code> 进行格式化，NameNode 就会随机生成一个 id。federation 模式下，如果每组 NameNode 格式化时都不指定 id，那每组 NameService 都会生成自己的 clusterid，进而导致每组之间都不一致。Datanode 初始化 block pool 时，会对比各组 NameService 的 clusterId，如果不一致则会出现该异常。</p>
<h3 id="解决方法-2"><a href="#解决方法-2" class="headerlink" title="解决方法"></a>解决方法</h3><p>在格式化每一组 NameNode 时，都指定统一的 clusterId 。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">首先选择一个独一无二 clusterId, 或者在第一组 NameNode 格式化时随机生成一个。</span><br><span class="line">后续格式化其余组 NameNode 时，都使用 -clusterid id 指定</span><br><span class="line"></span><br><span class="line">同一组 NameNode上，第一个 namenode 上使用format 进行格式化</span><br><span class="line">hdfs namenode -format -clusterid id</span><br><span class="line"></span><br><span class="line">第二个是用 bootstrapStandby 同步即可</span><br><span class="line">hdfs namenode -bootstrapStandby</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Hadoop-HDFS</category>
      </categories>
      <tags>
        <tag>HDFS</tag>
        <tag>trouble shooting</tag>
        <tag>运维</tag>
      </tags>
  </entry>
  <entry>
    <title>Yarn 使用 Cgroup 实现任务资源限制</title>
    <url>/2019/10/20/yarn-cgroup-usage/</url>
    <content><![CDATA[<p>Linux CGroup 全称是 Linux Control Group，是 Linux 内核提供的一个用来限制进程资源使用的功能，支持如 CPU, 内存，磁盘 IO 等资源的使用限制。用户可以使用 CGroup 对单个进程或者一组进程进行精细化的资源限制，具体使用方式可以查看参考文档。</p><p>目前， Yarn NodeManager 能够使用 CGroup 来限制所有 containers 的资源使用，主要是 CPU 资源。如果不用 CGroup， 在 NM 端很难实现对 container 的 CPU 使用进行限制。默认状态下， container 的 CPU 使用是没有限制的，container 申请了 1 vcore ，实际上能够使用所有的 CPU 资源。所以如果 NM 上分配了一个 vcore 申请较少实际上 CPU 使用极高的任务，常常会导致节点上运行的所有的任务都延时。</p><a id="more"></a>

<p>NM 运行时，可以通过 ContainersMonitor 线程监控 container 内存和 CPU 使用。对于 container 内存使用， 一旦发现其超出申请内存大小，就会立即发起 kill container 命令，回收 container 的资源。ContainersMonitor 虽然也支持 CPU 使用监控，但是 CPU 资源不像内存资源，其使用量的峰值是基本上可以确定的，在所有机器或者系统上都基本一致。 CPU 受限于 CPU 硬件性能， 同一个任务在不同的机器上的 CPU 使用率可能差异巨大，所以不能发现 container CPU 使用超过申请大小就 kill container 。同时，由于 container 都是由子进程的方式启动的， NM 也是很难通过直接控制 container 运行和暂停来调整其 CPU 使用率。 因此，在没有 CGroup 功能的情况下， NM 是很难直接限制 container 的 CPU 使用的。</p>
<p>所以接下来我们主要介绍 Yarn 如何启用 CGroup 来限制 containers CPU 资源占用。</p>
<h2 id="启用"><a href="#启用" class="headerlink" title="启用"></a>启用</h2><p>NM 启用 CGroup 功能主要需要在 <code>yarn-site.xml</code> 里设置以下配置：</p>
<h3 id="1-启用-LCE-："><a href="#1-启用-LCE-：" class="headerlink" title="1. 启用 LCE ："></a>1. 启用 LCE ：</h3><p>在 Nodemanager 中， CGroup 功能集成在 LinuxContainerExecutor 中，所以要使用 CGroup 功能，<strong>必须</strong>设置 container-executor 为 LinuxContainerExecutor. 同时需要配置 NM 的 Unix Group，这个是可执行的二进制文件 container-executor 用来做安全验证的，需要与 container-executor.cfg 里面配置的一致。 详细配置可参见 <a href="yarn-container-executour">Yarn ContainerExecutor 配置与使用</a>。</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.container-executor.class<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor<span class="tag">&lt;<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.linux-container-executor.group<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop<span class="tag">&lt;<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h3 id="2-启用-CGroup-："><a href="#2-启用-CGroup-：" class="headerlink" title="2. 启用 CGroup ："></a>2. 启用 CGroup ：</h3><p>LinuxContainerExecutor 并不会强制开启 CGroup 功能， 如果想要开启CGroup 功能，<strong>必须</strong>设置 resource-handler-class 为 CGroupsLCEResourceHandler.</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.linux-container-executor.resources-handler.class<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.yarn.server.nodemanager.util.CgroupsLCEResourcesHandler<span class="tag">&lt;<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h3 id="3-配置-Yarn-CGroup-目录："><a href="#3-配置-Yarn-CGroup-目录：" class="headerlink" title="3. 配置 Yarn CGroup 目录："></a>3. 配置 Yarn CGroup 目录：</h3><p>NM 通过 <code>yarn.nodemanager.linux-container-executor.cgroups.hierarchy</code> 配置所有 Yarn Containers 进程放置的 CGroup 目录。</p>
<p>如果系统的 CGroup 未挂载和配置，可以在系统上手动挂载和配置和启用 CGroup 功能，也可以通过设置<br><code>yarn.nodemanager.linux-container-executor.cgroups.mount</code> 为 true，同时设置 CGroup 挂载路径 <code>yarn.nodemanager.linux-container-executor.cgroups.mount-path</code> 来实现 NM 自动挂载 CGroup (不建议这样用，问题挺多)。</p>
<p>如果系统的 CGroup 已经挂载且配置完成，而且 Yarn 用户有 CGroup cpu 子目录的写入权限，NM 会在 cpu 目录下创建 <code>hadoop-yarn</code> 目录 ，如果该目录已经存在，保证 yarn 用户有写入权限即可。</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.linux-container-executor.cgroups.hierarchy<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>/hadoop-yarn<span class="tag">&lt;<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.linux-container-executor.cgroups.mount<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.linux-container-executor.cgroups.mount-path<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>/sys/fs/cgroup<span class="tag">&lt;<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>


<h3 id="4-CPU-资源限制："><a href="#4-CPU-资源限制：" class="headerlink" title="4. CPU 资源限制："></a>4. CPU 资源限制：</h3><p>NM 主要使用两个参数来限制 containers CPU 资源使用。</p>
<p>首先，使用 <code>yarn.nodemanager.resource.percentage-physical-cpu-limit</code> 来设置所有 containers 的总的 CPU 使用率占用总的 CPU 资源的百分比。比如设置为 60，则所有的 containers 的 CPU 使用总和在任何情况下都不会超过机器总体 CPU 资源的 60 %。</p>
<p>然后，使用 <code>yarn.nodemanager.linux-container-executor.cgroups.strict-resource-usage</code> 设置是否对 container 的 CPU 使用进行严格限制。如果设置为 true ，即便 NM 的 CPU 资源比较空闲， containers CPU 使用率也不能超过限制，这种配置下，可以严格限制 CPU 使用，保证每个 container 只能使用自己分配到的 CPU 资源。但是如果设置为 false ，container 可以在 NM 有空闲 CPU 资源时，超额使用 CPU，这种模式下，可以保证 NM 总体 CPU 使用率比较高，提升集群的计算性能和吞吐量，所以建议使用非严格的限制方式（实际通过 CGroup 的 cpu share 功能实现）。不论这个值怎么设置，所有 containers 总的 CPU 使用率都不会超过 cpu-limit 设置的值。</p>
<p>NM 会按照机器总的 <code>CPU num* limit-percent</code> 来计算 NM 总体可用的实际 CPU 资源，然后根据 NM 配置的 Vcore 数量来计算每个 Vcore 对应的实际 CPU 资源，再乘以 container 申请的 Vcore 数量计算 container 的实际可用的 CPU 资源。这里需要注意的是，在计算总体可用的 CPU 核数时，NM 默认使用的实际的物理核数，而一个物理核通常会对应多个逻辑核（单核多线程），而且我们默认的 CPU 核数通常都是逻辑核，所以我们需要设置 <code>yarn.nodemanager.resource.count-logical-processors-as-cores</code> 为 true 来指定使用逻辑核来计算 CPU 资源。</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.resource.percentage-physical-cpu-limit<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>80<span class="tag">&lt;<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.linux-container-executor.cgroups.strict-resource-usage<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span>    </span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.resource.count-logical-processors-as-cores<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>
<h2 id="注意事项"><a href="#注意事项" class="headerlink" title="注意事项"></a>注意事项</h2><p>Linux 内核版本 <code>3.10.0-327.el7.x86_64</code> 上 Yarn 启用 CGroup 功能后，会触发内核 BUG，导致内核卡死，重启，NM 挂掉，所有运行的任务失败。所以如果需要启用 CGroup 功能，绝对不能使用<code>3.10.0-327.el7.x86_64</code> 版本内核。亲测升级内核版本可解决该问题。</p>
<p>参见 <a href="https://runitao.github.io/a-process-to-reproduce-kernel-crash.html" target="_blank" rel="noopener">一次重现内核 Bug 的经历</a></p>
<h2 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h2><ol>
<li><a href="https://tech.meituan.com/2015/03/31/cgroups.html" target="_blank" rel="noopener">Linux资源管理之cgroups简介</a>    </li>
<li><a href="https://coolshell.cn/articles/17049.html" target="_blank" rel="noopener">DOCKER基础技术：LINUX CGROUP</a>   </li>
<li><a href="https://hadoop.apache.org/docs/r3.2.0/hadoop-yarn/hadoop-yarn-site/NodeManagerCgroups.html" target="_blank" rel="noopener">Using CGroups with YARN</a></li>
<li><a href="https://runitao.github.io/a-process-to-reproduce-kernel-crash.html" target="_blank" rel="noopener">一次重现内核 Bug 的经历</a></li>
</ol>
]]></content>
      <categories>
        <category>Hadoop-YARN</category>
      </categories>
      <tags>
        <tag>Yarn</tag>
        <tag>Cgroup</tag>
        <tag>资源限制</tag>
      </tags>
  </entry>
  <entry>
    <title>Yarn ContainerExecutor 配置与使用</title>
    <url>/2019/10/12/yarn-container-executor/</url>
    <content><![CDATA[<p>在 Yarn 的架构中，将集群中的计算资源，主要是内存和 CPU ，封装抽象出了 Container 的概念， 类似于 <code>container_001 &lt;memory:2048, vCores:1&gt;</code>。 Container 由 ResourceManager 负责调度与分配，由资源所在的 NodeManager 负责启动与管理。</p><a id="more"></a>
<p>Container 所封装的计算资源是由集群中的 NodeManager 提供的，所以 Container 的启动，运行，监控， 管理也需要对应的 NodeManager 来执行。 ContainerExecutor 正是 NodeManager 用来启动与管理 Container 的工具。</p>
<p>Yarn 3.0 版本中，在 Linux 系统环境下，ContainerExecutor 有两种实现：</p>
<ul>
<li>DefaultContainerExecutor:   简称 DCE ,如其名，是默认的 ContainerExecutor 实现。 如果用户未指定 ContainerExecutor 的具体实现，NM 就会使用它。 DCE 直接使用 bash 来启动 container 进程，所有 container 都使用 NM 进程用户 (yarn) 启动。</li>
</ul>
<ul>
<li>LinuxContainerExecutor:   简称 LCE，相比于 DCE ，它能提供更多有用的功能，如用户权限隔离，支持使用提交任务用户来启动 container；支持使用 cgroup 进行资源限制； 支持运行 docker container (合并了 2.x 版本中的 DockerContainerExecutor)。 LCE 使用可执行的二进制文件 container-executor 来启动 container 进程，container 的用户根据配置可以统一使用默认用户，也可以使用提交任务的用户（需要提前在 NM 上添加所有支持的用户）。</li>
</ul>
<p><strong>LCE 是安全的 ContainerExecutor</strong>。在默认的情况下，如果我们的集群是 non-secure ，而且没有什么特殊需求时，使用 DCE 就足够了，因为 DCE 配置和使用都很简单。但是当我们的集群要求安全性，比如开启了 kerberos 验证，我们就必须使用 LCE，使用 DCE 的话 MR 任务会在 reduce shuffle 阶段失败退出，参见<a href="https://issues.apache.org/jira/browse/YARN-1432" target="_blank" rel="noopener">YARN-1432</a>。</p>
<h2 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h2><p>NM 具体使用哪种 ContainerExecutor 的实现由 yarn-site.xml 里的 <code>yarn.nodemanager.container-executor.class</code> 属性来配置。</p>
<h3 id="DefaultContainerExecutor"><a href="#DefaultContainerExecutor" class="headerlink" title="DefaultContainerExecutor"></a>DefaultContainerExecutor</h3><p>DCE 的配置比较简单，只需要在 yarn-site.xml 指定 <code>yarn.nodemanager.container-executor.class</code> 属性为 <code>org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor</code> 即可，不需要额外的配置。</p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.container-executor.class<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h3 id="LinuxContainerExecutor"><a href="#LinuxContainerExecutor" class="headerlink" title="LinuxContainerExecutor"></a>LinuxContainerExecutor</h3><p>LCE 配置相对更加复杂一些， 具体如下：</p>
<p><strong>1. yarn-site.xml 配置</strong></p>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- NM 的 Unix 用户组，需要跟 container-executor.cfg 里面的配置一致，主要用来验证是否有安全访问 container-executor 二进制的权限 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.linux-container-executor.group<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 集群在 nonsecure 模式时，是否限制 container 的启动用户，true：container 使用统一的用户启动 false: container 使用任务用户启动 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.linux-container-executor.nonsecure-mode.limit-users<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 集群在 nonsecure 模式时，且开启 container 启动用户限制时，统一使用的用户，如果不设置，默认为 nobody --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.linux-container-executor.nonsecure-mode.local-user<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p><strong>2. container-executor.cfg 配置</strong></p>
<p>container-executor.cfg 是 container-executor 二进制程序的配置文件，会在其启动时读取校验。<br>具体的属性如下：</p>
<ul>
<li>yarn.nodemanager.linux-container-executor.group: NM 的 Unix 用户组， 需要与 yarn-site.xml 里一致。</li>
<li>allowed.system.users: 允许使用的系统用户，多个用户使用 ‘,’ 分隔，可以不设置，即允许所有用户。</li>
<li>banned.users: 禁止使用的用户，多个用户使用 ‘,’ 分隔。</li>
<li>min.user.id: 允许使用的用户的 uid 最小值，防止有其他超级用户。</li>
</ul>
<p>配置样例如下：</p>
<figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="meta">yarn.nodemanager.linux-container-executor.group</span>=<span class="string">hadoop</span></span><br><span class="line"><span class="meta">allowed.system.users</span>=<span class="string">hdfs,yarn,mapred,</span></span><br><span class="line"><span class="meta">banned.users</span>=<span class="string">root,bin</span></span><br><span class="line"><span class="meta">min.user.id</span>=<span class="string">100</span></span><br></pre></td></tr></table></figure>

<h2 id="启用"><a href="#启用" class="headerlink" title="启用"></a>启用</h2><p>DCE 的启用不需要额外的操作，配置完成后直接重启 NM 即可。LCE 的启用则还需要一系列的准备工作。</p>
<p><strong>1. 设置 container-executor 权限</strong>    </p>
<p>container-executor 二进制的 owner 必须是 root，属组必须与 NM 属组相同 (hadoop)，同时，它的权限必须设置成为 6050，以赋予它 setuid 的权限，来实现使用不同的用户来启动 container。</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">chown root:hadoop /usr/lib/hadoop-yarn/bin/container-executor </span><br><span class="line">chmod 6050 /usr/lib/hadoop-yarn/bin/container-executor</span><br></pre></td></tr></table></figure>

<p><strong>2. 设置 container-executor.cfg 权限</strong>    </p>
<p>container-executor.cfg 二进制的 owner 必须是 root，属组必须与 NM 属组相同 (hadoop)，同时，它的权限必须设置成为 0400， 以保证它是只读不可写的</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">chown root:hadoop /etc/hadoop/conf/container-executor.cfg</span><br><span class="line">chmode 0400 /etc/hadoop/conf/container-executor.cfg</span><br></pre></td></tr></table></figure>

<p><strong>3. 设置 yarn-local 和 yarn-log dirs 权限</strong>    </p>
<p>所有配置的 yarn-local 和 yarn-log 目录属主必须是 yarn:hadoop ，这个一般集群搭建时已经修改好了，不需要再处理。</p>
<p><strong>4. 任务用户管理</strong>    </p>
<p>在<strong>集群未开启 security authentication 时，也就是集群的用户校验方式为 <code>hadoop.security.authentication=simple</code> 时</strong>，如果开启用户限制<code>yarn.nodemanager.linux-container-executor.nonsecure-mode.limit-users=false</code>，也就是说使用任务提交用户来运行其 container 时，则需要提前在集群所有的 NM 节点上将需要的用户通过 useradd 命令，逐个添加到机器上，否则任务运行时会因为找不到指定用户而失败。当集群规模比较大，用户很多时，添加用户还是比较繁琐的，也不好维护，所以以我们的实践经验来看，建议开启用户限制，统一使用 yarn 用户来启动 container。</p>
<p>不过当集群在 security 模式时，基本上就是开启kerberos 做权限验证时，只允许使用任务提交用户来启动 container，这时就比较苦逼的需要把 YARN 集群里的用户都添加到 NM 节点上才行。不过为了不这么麻烦，我对这块逻辑进行了修改，允许在安全下依然可以使用统一 yarn 用户来执行用户任务。具体实现方式可以查看<a href="yarn-limit-user-in-secure-mode">YARN 在安全模式下使用统一用户运行 container</a>。</p>
<p>当上述的操作都完成后，重启 NM ， LCE 就会被启用。</p>
<h2 id="注意事项"><a href="#注意事项" class="headerlink" title="注意事项"></a>注意事项</h2><p>container-executor 和 container-executor.cfg 文件的权限一定要按照要求修改，否则 NM 会启动失败。如果改了权限后，NM 还是报权限问题，还需要将文件所在的目录用户组也改成 root:hadoop，权限改成 755。</p>
<h2 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h2><ul>
<li><a href="https://www.ibm.com/support/knowledgecenter/en/SSPT3X_4.2.0/com.ibm.swg.im.infosphere.biginsights.install.doc/doc/inst_adv_yarn_config.html" target="_blank" rel="noopener">Configuring YARN container executor</a></li>
<li><a href="https://hadoop.apache.org/docs/r3.2.0/hadoop-yarn/hadoop-yarn-site/SecureContainer.html" target="_blank" rel="noopener">YARN Secure Containers</a></li>
<li><a href="https://makeling.github.io/bigdata/dcb921f7.html" target="_blank" rel="noopener">理解和配置LinuxContainerExecutor
</a></li>
</ul>
]]></content>
      <categories>
        <category>Hadoop-YARN</category>
      </categories>
      <tags>
        <tag>Yarn</tag>
        <tag>ContainerExecutor</tag>
        <tag>LinuxContainerExecutor</tag>
      </tags>
  </entry>
  <entry>
    <title>写在最开始的话</title>
    <url>/2019/10/08/hello-world/</url>
    <content><![CDATA[<p>懒了这么久，终于还是下定决心开始写博客了。</p><p>博客的内容主要就是总结一下日常工作和学习中遇到的知识点和经验。目前而言，主要包括基于 Hadoop 生态的大数据开发相关的技术。写博客的主要目的是想将自己的技术栈以博客的形式记录沉淀下来，希望通过写博客的方式将接触到知识点梳理的更加清晰，加深自己的理解，找出自身的不足，更好的鞭策自己不断前行，进而达到提升自我的目的。 如果同时能给有想要了解相关知识的朋友们带来一点点帮助，那就更好不过了。</p><a id="more"></a>

<p>九层之台，起于累土，千里之行，始于足下。希望自己能坚持下来。</p>
<p>Keep coding, keep learning.</p>
]]></content>
      <categories>
        <category>其他</category>
      </categories>
  </entry>
</search>
